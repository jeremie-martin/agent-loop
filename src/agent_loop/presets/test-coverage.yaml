name: test-coverage
description: Add tests for untested critical paths with self-orchestrated verification

prompt_prefix: |
  FIRST: Find and read all markdown documentation in this project before doing anything else.
  Search for and read: README files (root and subdirectories), CLAUDE.md, AGENTS.md, and all
  files in docs/ or documentation/ directories. Understand the codebase's patterns, conventions,
  and architecture. Only then proceed with your task. Sub-agents you spawn must do the same.

prompt_suffix: |
  Do not use the "question" tool or any tool requiring user input.
  Stage and commit your changes at the end with a descriptive message.

  CRITICAL - When a test you write fails:
  1. First check: Is your test wrong? (wrong assertion, bad setup, misunderstood API)
     -> If yes, fix your test.
  2. If your test logic is correct, you may have found a bug in the implementation.
     -> Do NOT "fix" your test to match broken behavior.
     -> Do NOT delete the test.
     -> Mark it as skipped with a clear reason: @pytest.mark.skip(reason="Reveals bug: [description]")
     -> This preserves the test as documentation while not breaking CI.

  Never weaken or delete a correct test just because it fails. A failing test that catches
  a real bug is valuable—it just needs the implementation fixed, not the test.

modes:
  - name: coverage
    prompt: |
      Find meaningful gaps in test coverage and write tests that matter.

      Tests exist to catch bugs and document behavior. More tests are not better—
      better tests are better. A small suite that covers critical paths beats a
      large suite that tests trivia.

      Survey the codebase to understand:
      - What code paths handle user input, external data, or money?
      - What logic has complex branching or edge cases?
      - What has broken before (check git history for bug fixes)?

      Compare against the existing test suite. What critical behaviors lack tests?

      Prioritize ruthlessly by:
      1. Blast radius if broken (user-facing > internal, data loss > cosmetic)
      2. Complexity (tricky logic > straightforward CRUD)
      3. Change frequency (often-modified code > stable code)

      Pick the highest-priority gaps. Write tests that would catch real bugs—
      including edge cases (zero, one, many; empty input; boundaries).

      After writing tests, run them. If a test fails, determine whether your test
      is wrong or you've found an implementation bug. Fix incorrect tests; mark
      bug-revealing tests as skipped with a clear reason.

      "No new tests needed" is a valid outcome if the critical paths are covered.

      **After completing your changes**, orchestrate verification:

      1. Spawn a sub-agent to review your work. Give it:
         - This task (test coverage improvement)
         - The test files you created or modified
         - Instruction to check: do tests cover meaningful behavior? Would they
           catch real bugs? Are any redundant with existing tests?

      2. Spawn a filter sub-agent. Give it the verification feedback and ask:
         which concerns are genuine vs over-cautious?

      3. Address filtered feedback, then commit.

  - name: consolidation
    prompt: |
      Ensure the test suite remains lean and focused.

      A lean test suite runs faster and fails more clearly. Review for:
      - Tests that verify the same behavior in different words
      - Tests that would fail together (if one fails, the other always fails too)
      - Tests that check implementation details rather than behavior
      - Tests with excessive setup that obscures what's being tested

      Consolidate where possible:
      - Merge tests that check the same behavior
      - Delete tests that duplicate others
      - Simplify tests with clearer assertions

      Fewer, better tests. Deleting a redundant test is as valuable as adding
      a missing one.

      Run the suite after consolidation to confirm nothing broke.

      **After completing your changes**, orchestrate verification:

      1. Spawn a sub-agent to review your work. Give it:
         - This task (test consolidation)
         - The test files you modified
         - Instruction to check: did consolidation preserve test coverage?
           Are any behaviors now untested? Any further consolidation possible?

      2. Spawn a filter sub-agent. Give it the verification feedback and ask:
         which concerns are genuine vs over-cautious?

      3. Address filtered feedback, then commit.
