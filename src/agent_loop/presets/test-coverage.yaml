name: test-coverage
description: Add tests for untested critical paths with self-orchestrated verification

prompt_prefix: |
  FIRST: Find and read all markdown documentation in this project before doing anything else.
  Search for and read: README files (root and subdirectories), CLAUDE.md, AGENTS.md, and all
  files in docs/ or documentation/ directories. Understand the codebase's patterns, conventions,
  and architecture. Only then proceed with your task.

  When spawning sub-agents, explicitly instruct each one to first read all project
  documentation (README files, CLAUDE.md, AGENTS.md, docs/ and documentation/ directories)
  before starting its task.

prompt_suffix: |
  Do not use the "question" tool or any tool requiring user input.
  Stage and commit your changes at the end with a descriptive message.

  CRITICAL - When a test you write fails:
  1. First check: Is your test wrong? (wrong assertion, bad setup, misunderstood API)
     -> If yes, fix your test.
  2. If your test logic is correct, you may have found a bug in the implementation.
     -> Do NOT "fix" your test to match broken behavior.
     -> Do NOT delete the test.
     -> Mark it as skipped with a clear reason: @pytest.mark.skip(reason="Reveals bug: [description]")
     -> This preserves the test as documentation while not breaking CI.

  Never weaken or delete a correct test just because it fails. A failing test that catches
  a real bug is valuable—it just needs the implementation fixed, not the test.

modes:
  - name: coverage
    prompt: |
      Find meaningful gaps in test coverage and write tests that matter.

      Tests exist to catch bugs and document behavior. More tests are not better—
      better tests are better. A small suite that covers critical paths beats a
      large suite that tests trivia.

      Focus on code where bugs would hurt most: user-facing features, data handling,
      complex logic, and code that has broken before. Write tests that would actually
      catch bugs, including edge cases.

      After writing tests, run them. If a test fails, determine whether your test
      is wrong or you've found an implementation bug. Fix incorrect tests; mark
      bug-revealing tests as skipped with a clear reason.

      "No new tests needed" is a valid outcome if the critical paths are covered.

      **After completing your changes**, orchestrate verification:

      1. Spawn a sub-agent to review your work. Give it:
         - This task (test coverage improvement)
         - The test files you created or modified
         - Instruction to check: do tests cover meaningful behavior? Would they
           catch real bugs? Are any redundant with existing tests?

      2. Spawn a filter sub-agent. Give it the verification feedback and ask:
         which concerns are genuine vs over-cautious?

      3. Address filtered feedback, then commit.

  - name: consolidation
    prompt: |
      Ensure the test suite remains lean and focused.

      A lean test suite runs faster and fails more clearly. Look for tests that
      overlap, tests that check implementation details rather than behavior, and
      tests with excessive setup that obscures what's being tested.

      Fewer, better tests. Deleting a redundant test is as valuable as adding
      a missing one.

      Run the suite after consolidation to confirm nothing broke.

      **After completing your changes**, orchestrate verification:

      1. Spawn a sub-agent to review your work. Give it:
         - This task (test consolidation)
         - The test files you modified
         - Instruction to check: did consolidation preserve test coverage?
           Are any behaviors now untested? Any further consolidation possible?

      2. Spawn a filter sub-agent. Give it the verification feedback and ask:
         which concerns are genuine vs over-cautious?

      3. Address filtered feedback, then commit.
