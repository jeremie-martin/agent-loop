name: error-handling
description: Establish consistent error handling patterns with self-orchestrated verification

prompt_prefix: |
  FIRST: Find and read all markdown documentation in this project before doing anything else.
  Search for and read: README files (root and subdirectories), CLAUDE.md, AGENTS.md, and all
  files in docs/ or documentation/ directories. Understand the codebase's patterns, conventions,
  and architecture. Only then proceed with your task.

  When spawning sub-agents, explicitly instruct each one to first read all project
  documentation (README files, CLAUDE.md, AGENTS.md, docs/ and documentation/ directories)
  before starting its task.

prompt_suffix: |
  Do not use the "question" tool or any tool requiring user input.
  Stage and commit your changes at the end with a descriptive message.

modes:
  - name: patterns
    prompt: |
      Survey and improve error handling patterns in this codebase.

      Before changing anything, understand what exists. Survey for:
      - What exception types are raised? (built-in, custom, library-specific)
      - How are errors caught? (bare except, specific types, exception groups)
      - Where do errors originate vs where are they handled?

      Identify where similar operations handle errors differently:
      - File operations that sometimes raise, sometimes return None
      - Network calls with varying timeout/retry behavior
      - Validation that uses exceptions in some places, return values in others

      For inconsistencies, choose the pattern that fits best and apply it uniformly.
      Prefer the pattern already used more often in the codebase.

      Keep exception types semantic: ValueError for bad inputs, RuntimeError for
      impossible states, custom types for domain-specific errors.

      Review error messages throughout:
      - Do they say what went wrong? ("Connection failed" vs "Error occurred")
      - Do they include relevant context? (which file, what value, what was expected)
      - For user-facing errors, do they suggest what to do next?

      Improve messages that are vague or unhelpful. Add context where it would help
      debugging. Avoid exposing internal implementation details in user-facing messages.

      **After completing your changes**, orchestrate verification:

      1. Spawn a sub-agent to review your work. Give it:
         - This task (error handling improvement)
         - The files you modified
         - Instruction to check: is error handling logic correct? No silent exception
           swallowing? Messages helpful and grammatically correct?

      2. Spawn a filter sub-agent. Give it the verification feedback and ask:
         which concerns are genuine vs opinions about exception hierarchy?

      3. Address filtered feedback, then commit.

  - name: propagation
    prompt: |
      Ensure error handling is consistent between raise sites and catch sites.

      When exception types or patterns change but callers aren't updated, code
      breaks silently or catches the wrong thing. Find and fix these mismatches.

      Survey the codebase for inconsistencies:

      **Catch blocks catching the wrong type**: Find except clauses and verify
      the caught type matches what's actually raised. Look for:
      - Catching a parent type when a more specific custom exception is raised
      - Catching exceptions that are no longer raised
      - Missing catches for newly raised exceptions

      **Tests asserting wrong exceptions**: Find tests using `pytest.raises` or
      similar and verify they match the actual exception types and messages.

      **Documentation mismatches**: Find docstrings or docs that describe error
      behavior and verify they match the implementation.

      For each mismatch found, update the catch site / test / doc to match the
      actual raise behavior. The raise site is the source of truth.

      Run tests after changes. Failing tests indicate fixes worked (if the test
      was wrong) or more propagation needed (if behavior changed).

      If everything is already consistent, "no changes needed" is a valid outcome.

      **After completing your changes**, orchestrate verification:

      1. Spawn a sub-agent to review your work. Give it:
         - The files you modified
         - Instruction to check: are catch blocks correct? Do tests pass?
           Any remaining mismatches?

      2. Spawn a filter sub-agent. Give it the verification feedback and ask:
         which concerns are genuine issues vs over-caution?

      3. Address filtered feedback, then commit.
